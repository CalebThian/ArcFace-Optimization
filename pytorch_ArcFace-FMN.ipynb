{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474e277c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f17963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import FaceMobileNet,myFaceMobileNet\n",
    "from model.metric import ArcFace, CosFace\n",
    "from model.loss import FocalLoss\n",
    "from dataset import load_data\n",
    "from config import Config as conf\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fffc4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\Course\\Graduate1\\Data_Science\\hw\\hw5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d090b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "dataloader, class_num = load_data(conf, training=True)\n",
    "embedding_size = conf.embedding_size\n",
    "device = conf.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08bf47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Setup\n",
    "net = FaceMobileNet(embedding_size).to(device)\n",
    "\n",
    "if conf.metric == 'arcface':\n",
    "    metric = ArcFace(embedding_size, class_num).to(device)\n",
    "else:\n",
    "    metric = CosFace(embedding_size, class_num).to(device)\n",
    "\n",
    "net = nn.DataParallel(net)\n",
    "metric = nn.DataParallel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "131a4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "if conf.loss == 'focal_loss':\n",
    "    criterion = FocalLoss(gamma=2)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if conf.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD([{'params': net.parameters()}, {'params': metric.parameters()}], \n",
    "                            lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "else:\n",
    "    optimizer = optim.Adam([{'params': net.parameters()}, {'params': metric.parameters()}],\n",
    "                            lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=conf.lr_step, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9039da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints Setup\n",
    "os.makedirs(conf.checkpoints, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5668236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/30:  14%|#3        | 977/7119 [02:44<17:15,  5.93it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26772\\2560306928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\grape\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\grape\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "net.train()\n",
    "\n",
    "#Test at the same time\n",
    "images = unique_image(conf.test_list)\n",
    "images = [osp.join(conf.test_root, img) for img in images]\n",
    "groups = group_image(images, conf.test_batch_size)\n",
    "feature_dict = dict()\n",
    "\n",
    "for e in range(conf.epoch):\n",
    "    #if e<=20:\n",
    "    #    continue\n",
    "    for data, labels in tqdm(dataloader, desc=f\"Epoch {e}/{conf.epoch}\",\n",
    "                             ascii=True, total=len(dataloader)):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embeddings = net(data)\n",
    "        thetas = metric(embeddings, labels)\n",
    "        loss = criterion(thetas, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {e}/{conf.epoch}, Loss: {loss}\")\n",
    "    # Test\n",
    "    net.eval()\n",
    "    feature_dict = dict()\n",
    "    for group in groups:\n",
    "        d = featurize(group, conf.test_transform, net, conf.device,low_light = False)\n",
    "        feature_dict.update(d) \n",
    "    accuracy, threshold = compute_accuracy(feature_dict, conf.test_list, conf.test_root) \n",
    "\n",
    "    print(\n",
    "        f\"Test Model: FMN_{e}_sample{int(conf.train_sample_rate*100)}%.pth\\n\",\n",
    "        f\"Accuracy: {accuracy:.3f}\\n\"\n",
    "        f\"Threshold: {threshold:.3f}\\n\"\n",
    "    )\n",
    "    net.train()\n",
    "    backbone_path = osp.join(conf.checkpoints, f\"myFMN_{e}_sample{int(conf.train_sample_rate*100)}%.pth\")\n",
    "    torch.save(net.state_dict(), backbone_path)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280358e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7582ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_image(pair_list) -> set:\n",
    "    \"\"\"Return unique image path in pair_list.txt\"\"\"\n",
    "    with open(pair_list, 'r') as fd:\n",
    "        pairs = fd.readlines()\n",
    "    unique = set()\n",
    "    for pair in pairs:\n",
    "        id1, id2, _ = pair.split()\n",
    "        unique.add(id1)\n",
    "        unique.add(id2)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e031753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_image(images: set, batch) -> list:\n",
    "    \"\"\"Group image paths by batch size\"\"\"\n",
    "    images = list(images)\n",
    "    size = len(images)\n",
    "    res = []\n",
    "    for i in range(0, size, batch):\n",
    "        end = min(batch + i, size)\n",
    "        res.append(images[i : end])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24298711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(images: list, transform, low_light = False) -> torch.Tensor:\n",
    "    res = []\n",
    "    for img in images:\n",
    "        im = Image.open(img)\n",
    "        if low_light:\n",
    "            im = (np.sqrt (im)*2).astype(np.uint8)\n",
    "            im = Image.fromarray(im)\n",
    "        im = transform(im)\n",
    "        res.append(im)\n",
    "    data = torch.cat(res, dim=0)  # shape: (batch, 128, 128)\n",
    "    data = data[:, None, :, :]    # shape: (batch, 1, 128, 128)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9156a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(images: list, transform, net, device,low_light = False) -> dict:\n",
    "    \"\"\"featurize each image and save into a dictionary\n",
    "    Args:\n",
    "        images: image paths\n",
    "        transform: test transform\n",
    "        net: pretrained model\n",
    "        device: cpu or cuda\n",
    "    Returns:\n",
    "        Dict (key: imagePath, value: feature)\n",
    "    \"\"\"\n",
    "    data = _preprocess(images, transform,low_light = low_light)\n",
    "    data = data.to(device)\n",
    "    net = net.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = net(data) \n",
    "    res = {img: feature for (img, feature) in zip(images, features)}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30255a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosin_metric(x1, x2):\n",
    "    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ced99a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_score, y_true):\n",
    "    y_score = np.asarray(y_score)\n",
    "    y_true = np.asarray(y_true)\n",
    "    best_acc = 0\n",
    "    best_th = 0\n",
    "    for i in range(len(y_score)):\n",
    "        th = y_score[i]\n",
    "        y_test = (y_score >= th)\n",
    "        acc = np.mean((y_test == y_true).astype(int))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_th = th\n",
    "    return best_acc, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9dd691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(feature_dict, pair_list, test_root):\n",
    "    with open(pair_list, 'r') as f:\n",
    "        pairs = f.readlines()\n",
    "\n",
    "    similarities = []\n",
    "    labels = []\n",
    "    for pair in pairs:\n",
    "        img1, img2, label = pair.split()\n",
    "        img1 = osp.join(test_root, img1)\n",
    "        img2 = osp.join(test_root, img2)\n",
    "        feature1 = feature_dict[img1].cpu().numpy()\n",
    "        feature2 = feature_dict[img2].cpu().numpy()\n",
    "        label = int(label)\n",
    "\n",
    "        similarity = cosin_metric(feature1, feature2)\n",
    "        similarities.append(similarity)\n",
    "        labels.append(label)\n",
    "\n",
    "    accuracy, threshold = threshold_search(similarities, labels)\n",
    "    return accuracy, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff384ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Model: checkpoints/26.pth\n",
      "Accuracy: 0.945\n",
      "Threshold: 0.308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = \"checkpoints/26.pth\" #conf.test_model\n",
    "model = FaceMobileNet(conf.embedding_size)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(test_model, map_location=conf.device))\n",
    "model.eval()\n",
    "\n",
    "images = unique_image(conf.test_list)\n",
    "images = [osp.join(conf.test_root, img) for img in images]\n",
    "groups = group_image(images, conf.test_batch_size)\n",
    "\n",
    "feature_dict = dict()\n",
    "for group in groups:\n",
    "    d = featurize(group, conf.test_transform, model, conf.device,low_light = False)\n",
    "    feature_dict.update(d) \n",
    "accuracy, threshold = compute_accuracy(feature_dict, conf.test_list, conf.test_root) \n",
    "\n",
    "print(\n",
    "    f\"Test Model: {test_model}\\n\"\n",
    "    f\"Accuracy: {accuracy:.3f}\\n\"\n",
    "    f\"Threshold: {threshold:.3f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b68cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Model: checkpoints/0.pth\n",
      "Accuracy: 0.846\n",
      "Threshold: 0.445\n",
      "\n",
      "Test Model: checkpoints/1.pth\n",
      "Accuracy: 0.883\n",
      "Threshold: 0.422\n",
      "\n",
      "Test Model: checkpoints/2.pth\n",
      "Accuracy: 0.886\n",
      "Threshold: 0.372\n",
      "\n",
      "Test Model: checkpoints/3.pth\n",
      "Accuracy: 0.889\n",
      "Threshold: 0.393\n",
      "\n",
      "Test Model: checkpoints/4.pth\n",
      "Accuracy: 0.906\n",
      "Threshold: 0.341\n",
      "\n",
      "Test Model: checkpoints/5.pth\n",
      "Accuracy: 0.904\n",
      "Threshold: 0.396\n",
      "\n",
      "Test Model: checkpoints/6.pth\n",
      "Accuracy: 0.904\n",
      "Threshold: 0.347\n",
      "\n",
      "Test Model: checkpoints/7.pth\n",
      "Accuracy: 0.876\n",
      "Threshold: 0.453\n",
      "\n",
      "Test Model: checkpoints/8.pth\n",
      "Accuracy: 0.912\n",
      "Threshold: 0.373\n",
      "\n",
      "Test Model: checkpoints/9.pth\n",
      "Accuracy: 0.918\n",
      "Threshold: 0.377\n",
      "\n",
      "Test Model: checkpoints/10.pth\n",
      "Accuracy: 0.940\n",
      "Threshold: 0.308\n",
      "\n",
      "Test Model: checkpoints/11.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.290\n",
      "\n",
      "Test Model: checkpoints/12.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.320\n",
      "\n",
      "Test Model: checkpoints/13.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.291\n",
      "\n",
      "Test Model: checkpoints/14.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.303\n",
      "\n",
      "Test Model: checkpoints/15.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.304\n",
      "\n",
      "Test Model: checkpoints/16.pth\n",
      "Accuracy: 0.937\n",
      "Threshold: 0.328\n",
      "\n",
      "Test Model: checkpoints/17.pth\n",
      "Accuracy: 0.941\n",
      "Threshold: 0.301\n",
      "\n",
      "Test Model: checkpoints/18.pth\n",
      "Accuracy: 0.939\n",
      "Threshold: 0.320\n",
      "\n",
      "Test Model: checkpoints/19.pth\n",
      "Accuracy: 0.941\n",
      "Threshold: 0.293\n",
      "\n",
      "Test Model: checkpoints/20.pth\n",
      "Accuracy: 0.945\n",
      "Threshold: 0.287\n",
      "\n",
      "Test Model: checkpoints/21.pth\n",
      "Accuracy: 0.947\n",
      "Threshold: 0.295\n",
      "\n",
      "Test Model: checkpoints/22.pth\n",
      "Accuracy: 0.948\n",
      "Threshold: 0.292\n",
      "\n",
      "Test Model: checkpoints/23.pth\n",
      "Accuracy: 0.946\n",
      "Threshold: 0.289\n",
      "\n",
      "Test Model: checkpoints/24.pth\n",
      "Accuracy: 0.946\n",
      "Threshold: 0.283\n",
      "\n",
      "Test Model: checkpoints/25.pth\n",
      "Accuracy: 0.945\n",
      "Threshold: 0.280\n",
      "\n",
      "Test Model: checkpoints/26.pth\n",
      "Accuracy: 0.945\n",
      "Threshold: 0.308\n",
      "\n",
      "Test Model: checkpoints/27.pth\n",
      "Accuracy: 0.945\n",
      "Threshold: 0.289\n",
      "\n",
      "Test Model: checkpoints/28.pth\n",
      "Accuracy: 0.946\n",
      "Threshold: 0.288\n",
      "\n",
      "Test Model: checkpoints/29.pth\n",
      "Accuracy: 0.947\n",
      "Threshold: 0.287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    test_model = \"checkpoints/\"+str(i)+\".pth\" #conf.test_model\n",
    "    model = FaceMobileNet(conf.embedding_size)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(test_model, map_location=conf.device))\n",
    "    model.eval()\n",
    "\n",
    "    images = unique_image(conf.test_list)\n",
    "    images = [osp.join(conf.test_root, img) for img in images]\n",
    "    groups = group_image(images, conf.test_batch_size)\n",
    "\n",
    "    feature_dict = dict()\n",
    "    for group in groups:\n",
    "        d = featurize(group, conf.test_transform, model, conf.device,low_light = False)\n",
    "        feature_dict.update(d) \n",
    "    accuracy, threshold = compute_accuracy(feature_dict, conf.test_list, conf.test_root) \n",
    "\n",
    "    print(\n",
    "        f\"Test Model: {test_model}\\n\"\n",
    "        f\"Accuracy: {accuracy:.3f}\\n\"\n",
    "        f\"Threshold: {threshold:.3f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88720da",
   "metadata": {},
   "source": [
    "# Get #params and FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e66505ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module ConvBn is treated as a zero-op.\n",
      "Warning: module ConvBnPrelu is treated as a zero-op.\n",
      "Warning: module DepthWise is treated as a zero-op.\n",
      "Warning: module DepthWiseRes is treated as a zero-op.\n",
      "Warning: module MultiDepthWiseRes is treated as a zero-op.\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module myFaceMobileNet is treated as a zero-op.\n",
      "Warning: module DataParallel is treated as a zero-op.\n",
      "Computational complexity:       474061824.0\n",
      "Number of parameters:           1096896 \n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(0):\n",
    "    net = model\n",
    "    macs, params = get_model_complexity_info(net, (1,128,128), as_strings=False,\n",
    "                                           print_per_layer_stat=False, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs*2))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ab100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import myFaceMobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954ec74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myFaceMobileNet(conf.embedding_size)\n",
    "model = nn.DataParallel(model)\n",
    "#model.load_state_dict(torch.load(test_model, map_location=conf.device))\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981e39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
